package com.lhzkml.jasmine.core.agent.graph.graph

import com.lhzkml.jasmine.core.prompt.llm.HistoryCompressionStrategy
import com.lhzkml.jasmine.core.prompt.llm.StructuredResponse
import com.lhzkml.jasmine.core.prompt.llm.replaceHistoryWithTLDR
import com.lhzkml.jasmine.core.prompt.model.ChatResult
import com.lhzkml.jasmine.core.prompt.model.ToolCall
import com.lhzkml.jasmine.core.prompt.model.ToolDescriptor
import com.lhzkml.jasmine.core.prompt.model.latestTokenUsage
import kotlinx.serialization.KSerializer

/**
 * FunctionalContext æ‰©å±•å‡½æ•°
 * ç§»æ¤è‡?koog çš?AIAgentFunctionalContextExt.ktï¼?
 * ä¸?AgentGraphContext æä¾›ä¾¿æ·çš?LLM äº¤äº’å’Œå·¥å…·æ‰§è¡Œæ–¹æ³•ï¼Œ
 * å¤§å¹…æå‡ functionalStrategy çš„æ˜“ç”¨æ€§ã€?
 *
 * ä½¿ç”¨æ–¹å¼ï¼?
 * ```kotlin
 * val strategy = functionalStrategy<String, String>("chat") { input ->
 *     // ç›´æ¥ä½¿ç”¨æ‰©å±•å‡½æ•°ï¼Œæ— éœ€æ‰‹åŠ¨æ“ä½œ session
 *     val result = requestLLM(input)
 *     if (result.hasToolCalls) {
 *         val toolResults = executeMultipleTools(result.toolCalls)
 *         val finalResult = sendMultipleToolResults(toolResults)
 *         finalResult.content
 *     } else {
 *         result.content
 *     }
 * }
 * ```
 */

// ========== LLM è¯·æ±‚ ==========

/**
 * è¿½åŠ ç”¨æˆ·æ¶ˆæ¯å¹¶è¯·æ±?LLM
 * ç§»æ¤è‡?koog çš?AIAgentFunctionalContext.requestLLM
 *
 * @param message ç”¨æˆ·æ¶ˆæ¯å†…å®¹
 * @param allowToolCalls æ˜¯å¦å…è®¸å·¥å…·è°ƒç”¨ï¼Œé»˜è®?true
 * @return LLM å“åº”ç»“æœ
 */
suspend fun AgentGraphContext.requestLLM(
    message: String,
    allowToolCalls: Boolean = true
): ChatResult {
    session.appendPrompt { user(message) }
    return if (allowToolCalls) {
        session.requestLLM()
    } else {
        session.requestLLMWithoutTools()
    }
}

/**
 * è¿½åŠ ç”¨æˆ·æ¶ˆæ¯å¹¶æµå¼è¯·æ±?LLM
 *
 * @param message ç”¨æˆ·æ¶ˆæ¯å†…å®¹
 * @param onChunk æµå¼å†…å®¹å›è°ƒ
 * @param onThinking æ€è€ƒå†…å®¹å›è°?
 * @return LLM æµå¼å“åº”ç»“æœï¼ˆè½¬ä¸?ChatResultï¼?
 */
suspend fun AgentGraphContext.requestLLMStream(
    message: String,
    onChunk: suspend (String) -> Unit,
    onThinking: suspend (String) -> Unit = {}
): ChatResult {
    session.appendPrompt { user(message) }
    val streamResult = session.requestLLMStream(onChunk, onThinking)
    return ChatResult(
        content = streamResult.content,
        usage = streamResult.usage,
        finishReason = streamResult.finishReason,
        toolCalls = streamResult.toolCalls,
        thinking = streamResult.thinking
    )
}

/**
 * è¿½åŠ ç”¨æˆ·æ¶ˆæ¯å¹¶è¯·æ±?LLM è¿”å›ç»“æ„åŒ?JSON è¾“å‡º
 * ç§»æ¤è‡?koog çš?AIAgentFunctionalContext.requestLLMStructured
 *
 * @param message ç”¨æˆ·æ¶ˆæ¯å†…å®¹
 * @param serializer ç›®æ ‡ç±»å‹çš„åºåˆ—åŒ–å™?
 * @param examples å¯é€‰çš„ç¤ºä¾‹åˆ—è¡¨
 * @return Result åŒ…å«è§£æåçš„ StructuredResponse æˆ–é”™è¯?
 */
suspend fun <T> AgentGraphContext.requestLLMStructured(
    message: String,
    serializer: KSerializer<T>,
    examples: List<T> = emptyList()
): Result<StructuredResponse<T>> {
    session.appendPrompt { user(message) }
    return session.requestLLMStructured(serializer, examples)
}

/**
 * è¿½åŠ ç”¨æˆ·æ¶ˆæ¯å¹¶è¯·æ±?LLM è¿”å›ç»“æ„åŒ?JSON è¾“å‡ºï¼ˆinline reified ç‰ˆæœ¬ï¼?
 */
suspend inline fun <reified T> AgentGraphContext.requestLLMStructured(
    message: String,
    examples: List<T> = emptyList()
): Result<StructuredResponse<T>> {
    session.appendPrompt { user(message) }
    return session.requestLLMStructured<T>(examples)
}

/**
 * è¿½åŠ ç”¨æˆ·æ¶ˆæ¯å¹¶å¼ºåˆ?LLM ä½¿ç”¨æŒ‡å®šå·¥å…·
 * ç§»æ¤è‡?koog çš?AIAgentFunctionalContext.requestLLMForceOneTool
 *
 * @param message ç”¨æˆ·æ¶ˆæ¯å†…å®¹
 * @param toolName å¼ºåˆ¶ä½¿ç”¨çš„å·¥å…·åç§?
 * @return LLM å“åº”ç»“æœ
 */
suspend fun AgentGraphContext.requestLLMForceOneTool(
    message: String,
    toolName: String
): ChatResult {
    session.appendPrompt { user(message) }
    return session.requestLLMForceOneTool(toolName)
}

/**
 * è¿½åŠ ç”¨æˆ·æ¶ˆæ¯å¹¶å¼ºåˆ?LLM åªèƒ½è°ƒç”¨å·¥å…·ï¼ˆä¸èƒ½ç”Ÿæˆæ–‡æœ¬ï¼‰
 * ç§»æ¤è‡?koog çš?AIAgentFunctionalContext.requestLLMOnlyCallingTools
 *
 * @param message ç”¨æˆ·æ¶ˆæ¯å†…å®¹
 * @return LLM å“åº”ç»“æœ
 */
suspend fun AgentGraphContext.requestLLMOnlyCallingTools(
    message: String
): ChatResult {
    session.appendPrompt { user(message) }
    return session.requestLLMOnlyCallingTools()
}

// ========== å“åº”åˆ¤æ–­ ==========

/**
 * å¦‚æœ LLM å“åº”æ˜¯çº¯æ–‡æœ¬ï¼ˆæ— å·¥å…·è°ƒç”¨ï¼‰ï¼Œæ‰§è¡Œ action
 * ç§»æ¤è‡?koog çš?AIAgentFunctionalContext.onAssistantMessage
 *
 * @param result LLM å“åº”
 * @param action å½“å“åº”ä¸ºçº¯æ–‡æœ¬æ—¶æ‰§è¡Œçš„æ“ä½?
 */
inline fun AgentGraphContext.onAssistantMessage(
    result: ChatResult,
    action: (ChatResult) -> Unit
) {
    if (!result.hasToolCalls) {
        action(result)
    }
}

/**
 * å¦‚æœ LLM å“åº”åŒ…å«å·¥å…·è°ƒç”¨ï¼Œæ‰§è¡?action
 *
 * @param result LLM å“åº”
 * @param action å½“å“åº”åŒ…å«å·¥å…·è°ƒç”¨æ—¶æ‰§è¡Œçš„æ“ä½?
 */
inline fun AgentGraphContext.onToolCalls(
    result: ChatResult,
    action: (List<ToolCall>) -> Unit
) {
    if (result.hasToolCalls) {
        action(result.toolCalls)
    }
}

// ========== å·¥å…·æ‰§è¡Œ ==========

/**
 * æ‰§è¡Œå•ä¸ªå·¥å…·è°ƒç”¨
 * ç§»æ¤è‡?koog çš?AIAgentFunctionalContext.executeTool
 *
 * @param toolCall å·¥å…·è°ƒç”¨è¯·æ±‚
 * @return å·¥å…·æ‰§è¡Œç»“æœ
 */
suspend fun AgentGraphContext.executeTool(
    toolCall: ToolCall
): ReceivedToolResult {
    return environment.executeTool(toolCall)
}

/**
 * æ‰§è¡Œå¤šä¸ªå·¥å…·è°ƒç”¨
 * ç§»æ¤è‡?koog çš?AIAgentFunctionalContext.executeMultipleTools
 *
 * @param toolCalls å·¥å…·è°ƒç”¨è¯·æ±‚åˆ—è¡¨
 * @param parallelTools æ˜¯å¦å¹¶è¡Œæ‰§è¡Œï¼Œé»˜è®?false
 * @return å·¥å…·æ‰§è¡Œç»“æœåˆ—è¡¨
 */
suspend fun AgentGraphContext.executeMultipleTools(
    toolCalls: List<ToolCall>,
    parallelTools: Boolean = false
): List<ReceivedToolResult> {
    return if (parallelTools) {
        environment.executeTools(toolCalls)
    } else {
        toolCalls.map { environment.executeTool(it) }
    }
}

// ========== å·¥å…·ç»“æœå‘é€?==========

/**
 * å‘é€å•ä¸ªå·¥å…·ç»“æœå¹¶è¯·æ±‚ LLM
 * ç§»æ¤è‡?koog çš?AIAgentFunctionalContext.sendToolResult
 *
 * @param toolResult å·¥å…·æ‰§è¡Œç»“æœ
 * @return LLM å“åº”ç»“æœ
 */
suspend fun AgentGraphContext.sendToolResult(
    toolResult: ReceivedToolResult
): ChatResult {
    session.appendPrompt {
        tool {
            result(toolResult)
        }
    }
    return session.requestLLM()
}

/**
 * å‘é€å¤šä¸ªå·¥å…·ç»“æœå¹¶è¯·æ±‚ LLM
 * ç§»æ¤è‡?koog çš?AIAgentFunctionalContext.sendMultipleToolResults
 *
 * æ³¨æ„: koog åŸç‰ˆè¿”å› List<Message.Response>ï¼ˆè°ƒç”?requestLLMMultipleï¼‰ï¼Œ
 * jasmine é€‚é…ä¸ºè¿”å›?List<ChatResult>ã€?
 *
 * @param results å·¥å…·æ‰§è¡Œç»“æœåˆ—è¡¨
 * @return LLM å“åº”ç»“æœåˆ—è¡¨
 */
suspend fun AgentGraphContext.sendMultipleToolResults(
    results: List<ReceivedToolResult>
): List<ChatResult> {
    session.appendPrompt {
        tool {
            results.forEach { result(it) }
        }
    }
    return session.requestLLMMultiple()
}

// ========== å†å²å‹ç¼© ==========

/**
 * å‹ç¼©å½“å‰å¯¹è¯å†å²
 * ç§»æ¤è‡?koog çš?AIAgentFunctionalContext.compressHistory
 *
 * @param strategy å‹ç¼©ç­–ç•¥ï¼Œé»˜è®?WholeHistory
 * @param preserveMemory æ˜¯å¦ä¿ç•™è®°å¿†ç›¸å…³æ¶ˆæ¯ï¼Œé»˜è®?true
 */
suspend fun AgentGraphContext.compressHistory(
    strategy: HistoryCompressionStrategy = HistoryCompressionStrategy.WholeHistory,
    preserveMemory: Boolean = true
) {
    session.replaceHistoryWithTLDR(strategy, preserveMemory)
}

// ========== Token ç”¨é‡ ==========

/**
 * è·å–å½“å‰ prompt çš„ä¼°ç®?token æ•?
 *
 * @return ä¼°ç®—çš?token æ•?
 */
fun AgentGraphContext.estimateTokenUsage(): Int {
    return session.prompt.messages.sumOf { msg ->
        // ç²—ç•¥ä¼°ç®—: æ¯?ä¸ªå­—ç¬¦çº¦1ä¸ªtoken
        (msg.content.length / 4) + 4
    }
}

// ========== ä¸­ä¼˜å…ˆçº§æ‰©å±•å‡½æ•° ==========
// ç§»æ¤è‡?koog çš?AIAgentFunctionalContextExt.kt

/**
 * è¿½åŠ ç”¨æˆ·æ¶ˆæ¯å¹¶è¯·æ±?LLM è¿”å›å¤šä¸ªå“åº”
 * ç§»æ¤è‡?koog çš?AIAgentFunctionalContext.requestLLMMultiple
 *
 * @param message ç”¨æˆ·æ¶ˆæ¯å†…å®¹
 * @return LLM å“åº”ç»“æœåˆ—è¡¨
 */
suspend fun AgentGraphContext.requestLLMMultiple(
    message: String
): List<ChatResult> {
    session.appendPrompt { user(message) }
    return session.requestLLMMultiple()
}

/**
 * å¦‚æœ LLM å“åº”åˆ—è¡¨åŒ…å«å·¥å…·è°ƒç”¨ï¼Œæ‰§è¡?action
 * ç§»æ¤è‡?koog çš?AIAgentFunctionalContext.onMultipleToolCalls
 *
 * @param results LLM å“åº”åˆ—è¡¨
 * @param action å½“å“åº”åŒ…å«å·¥å…·è°ƒç”¨æ—¶æ‰§è¡Œçš„æ“ä½?
 */
inline fun AgentGraphContext.onMultipleToolCalls(
    results: List<ChatResult>,
    action: (List<ChatResult>) -> Unit
) {
    val toolCallResults = results.filter { it.hasToolCalls }
    if (toolCallResults.isNotEmpty()) {
        action(toolCallResults)
    }
}

/**
 * å¦‚æœ LLM å“åº”åˆ—è¡¨åŒ…å«çº¯åŠ©æ‰‹æ¶ˆæ¯ï¼Œæ‰§è¡Œ action
 * ç§»æ¤è‡?koog çš?AIAgentFunctionalContext.onMultipleAssistantMessages
 *
 * @param results LLM å“åº”åˆ—è¡¨
 * @param action å½“å“åº”åŒ…å«çº¯åŠ©æ‰‹æ¶ˆæ¯æ—¶æ‰§è¡Œçš„æ“ä½œ
 */
inline fun AgentGraphContext.onMultipleAssistantMessages(
    results: List<ChatResult>,
    action: (List<ChatResult>) -> Unit
) {
    val assistantResults = results.filter { !it.hasToolCalls }
    if (assistantResults.isNotEmpty()) {
        action(assistantResults)
    }
}

/**
 * æ£€æŸ¥å“åº”åˆ—è¡¨æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”?
 * ç§»æ¤è‡?koog çš?containsToolCalls
 *
 * @param results LLM å“åº”åˆ—è¡¨
 * @return æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨
 */
fun containsToolCalls(results: List<ChatResult>): Boolean {
    return results.any { it.hasToolCalls }
}

/**
 * ä»å“åº”åˆ—è¡¨ä¸­æå–æ‰€æœ‰å·¥å…·è°ƒç”?
 * ç§»æ¤è‡?koog çš?extractToolCalls
 *
 * @param results LLM å“åº”åˆ—è¡¨
 * @return æ‰€æœ‰å·¥å…·è°ƒç”¨çš„åˆ—è¡¨
 */
fun extractToolCalls(results: List<ChatResult>): List<ToolCall> {
    return results.flatMap { it.toolCalls }
}

/**
 * å‘é€å¤šä¸ªå·¥å…·ç»“æœå¹¶è·å–å¤šå“åº?
 *
 * @param results å·¥å…·æ‰§è¡Œç»“æœåˆ—è¡¨
 * @return LLM å¤šå“åº”ç»“æœåˆ—è¡?
 */
suspend fun AgentGraphContext.sendMultipleToolResultsMultiple(
    results: List<ReceivedToolResult>
): List<ChatResult> {
    session.appendPrompt {
        tool {
            results.forEach { result(it) }
        }
    }
    return session.requestLLMMultiple()
}

// ========== å“åº”ç±»å‹åˆ¤æ–­ ==========
// ç§»æ¤è‡?koog çš?asAssistantMessageOrNull / asAssistantMessage
// koog ç”¨å¯†å°ç±»å±‚æ¬¡(Message.Assistant/Tool.Call/Reasoning)ï¼Œjasmine ç”¨æ‰å¹?ChatResultã€?
// é€‚é…ä¸ºåˆ¤æ–?ChatResult æ˜¯å¦ä¸ºçº¯åŠ©æ‰‹æ¶ˆæ¯ï¼ˆæ— å·¥å…·è°ƒç”¨ï¼‰ã€?

/**
 * å¦‚æœ ChatResult æ˜¯çº¯åŠ©æ‰‹æ¶ˆæ¯ï¼ˆæ— å·¥å…·è°ƒç”¨ï¼‰ï¼Œè¿”å›è‡ªèº«ï¼›å¦åˆ™è¿”å›?null
 * ç§»æ¤è‡?koog çš?Message.Response.asAssistantMessageOrNull()
 *
 * @return ChatResult æˆ?null
 */
fun ChatResult.asAssistantMessageOrNull(): ChatResult? {
    return if (!hasToolCalls) this else null
}

/**
 * å¼ºåˆ¶å°?ChatResult è§†ä¸ºçº¯åŠ©æ‰‹æ¶ˆæ?
 * ç§»æ¤è‡?koog çš?Message.Response.asAssistantMessage()
 *
 * @return ChatResult
 * @throws IllegalStateException å¦‚æœåŒ…å«å·¥å…·è°ƒç”¨
 */
fun ChatResult.asAssistantMessage(): ChatResult {
    check(!hasToolCalls) { "ChatResult contains tool calls, not a pure assistant message" }
    return this
}

// ========== ç›´æ¥å·¥å…·è°ƒç”¨ ==========

/**
 * ç›´æ¥è°ƒç”¨æŒ‡å®šå·¥å…·ï¼ˆä¸ç»è¿‡ LLM é€‰æ‹©ï¼?
 * ç§»æ¤è‡?koog çš?AIAgentFunctionalContext.executeSingleTool
 *
 * koog åŸç‰ˆä½¿ç”¨ SafeTool<ToolArg, TResult> ç±»å‹åŒ–å‚æ•°ï¼Œjasmine é€‚é…ä¸ºå·¥å…·å + JSON å‚æ•°å­—ç¬¦ä¸²ã€?
 *
 * @param toolName è¦è°ƒç”¨çš„å·¥å…·åç§°
 * @param toolArgs å·¥å…·å‚æ•°ï¼ˆJSON æ ¼å¼å­—ç¬¦ä¸²ï¼‰
 * @param doAppendPrompt æ˜¯å¦å°†å·¥å…·è°ƒç”¨ä¿¡æ¯è¿½åŠ åˆ° promptï¼Œé»˜è®?true
 * @return å·¥å…·æ‰§è¡Œç»“æœ
 */
suspend fun AgentGraphContext.executeSingleTool(
    toolName: String,
    toolArgs: String,
    doAppendPrompt: Boolean = true
): ReceivedToolResult {
    if (doAppendPrompt) {
        session.appendPrompt {
            user("Tool call: $toolName was explicitly called with args: $toolArgs")
        }
    }

    val toolCall = ToolCall(
        id = "explicit_${toolName}_${System.currentTimeMillis()}",
        name = toolName,
        arguments = toolArgs
    )
    val result = environment.executeTool(toolCall)

    if (doAppendPrompt) {
        session.appendPrompt {
            user("Tool call: $toolName was explicitly called and returned result: ${result.content}")
        }
    }

    return result
}

// ========== Token ç”¨é‡ï¼ˆå®é™…å€¼ï¼‰ ==========

/**
 * è·å–æœ€æ–°çš„ token ç”¨é‡
 * ç§»æ¤è‡?koog çš?AIAgentFunctionalContext.latestTokenUsage
 *
 * koog ä»?prompt.latestTokenUsage è¯»å–å®é™…å€¼ï¼Œjasmine ä»æœ€è¿‘ä¸€æ¬?ChatResult çš?usage è·å–ã€?
 *
 * @param lastResult æœ€è¿‘ä¸€æ¬?LLM å“åº”ï¼ˆå¯é€‰ï¼‰
 * @return token ç”¨é‡ï¼Œå¦‚æœæ— æ³•è·å–è¿”å›?null
 */
fun AgentGraphContext.latestTokenUsage(lastResult: ChatResult? = null): Int? {
    return lastResult?.usage?.totalTokens
}


// ========== Message ç±»å‹çš„æ‰©å±•å‡½æ•?==========
// ç§»æ¤è‡?koog çš„ç±»å‹åŒ–æ¶ˆæ¯ç³»ç»Ÿ

/**
 * è¿½åŠ ç”¨æˆ·æ¶ˆæ¯å¹¶è¯·æ±?LLMï¼Œè¿”å›?Message.Response
 */
suspend fun AgentGraphContext.requestLLMAsMessage(
    message: String
): com.lhzkml.jasmine.core.prompt.model.Message.Response {
    session.appendPrompt { user(message) }
    return session.requestLLMAsMessage()
}

/**
 * è¿½åŠ ç”¨æˆ·æ¶ˆæ¯å¹¶è¯·æ±?LLMï¼ˆä¸å¸¦å·¥å…·ï¼‰ï¼Œè¿”å›?Message.Response
 */
suspend fun AgentGraphContext.requestLLMWithoutToolsAsMessage(
    message: String
): com.lhzkml.jasmine.core.prompt.model.Message.Response {
    session.appendPrompt { user(message) }
    return session.requestLLMWithoutToolsAsMessage()
}

/**
 * è¿½åŠ  Message ç±»å‹çš„æ¶ˆæ¯åˆ° prompt
 */
fun AgentGraphContext.appendMessage(message: com.lhzkml.jasmine.core.prompt.model.Message) {
    session.appendMessage(message)
}

/**
 * æ‰¹é‡è¿½åŠ  Message ç±»å‹çš„æ¶ˆæ¯åˆ° prompt
 */
fun AgentGraphContext.appendMessages(messages: List<com.lhzkml.jasmine.core.prompt.model.Message>) {
    session.appendMessages(messages)
}

// ========== Message.Response ç‰ˆæœ¬çš„å“åº”åˆ¤æ–?==========
// ç§»æ¤è‡?koog çš?AIAgentFunctionalContextExt.kt (Message.Response ç‰ˆæœ¬)

/**
 * å¦‚æœ Message.Response æ˜?Assistant æ¶ˆæ¯ï¼Œæ‰§è¡?action
 * ç§»æ¤è‡?koog çš?onAssistantMessage(Message.Response, (Message.Assistant) -> Unit)
 */
inline fun AgentGraphContext.onAssistantMessage(
    response: com.lhzkml.jasmine.core.prompt.model.Message.Response,
    action: (com.lhzkml.jasmine.core.prompt.model.Message.Assistant) -> Unit
) {
    if (response is com.lhzkml.jasmine.core.prompt.model.Message.Assistant) {
        action(response)
    }
}

/**
 * å¦‚æœ Message.Response åˆ—è¡¨åŒ…å« Tool.Callï¼Œæ‰§è¡?action
 * ç§»æ¤è‡?koog çš?onMultipleToolCalls(List<Message.Response>, (List<Message.Tool.Call>) -> Unit)
 */
inline fun AgentGraphContext.onMultipleToolCallMessages(
    responses: List<com.lhzkml.jasmine.core.prompt.model.Message.Response>,
    action: (List<com.lhzkml.jasmine.core.prompt.model.Message.Tool.Call>) -> Unit
) {
    val toolCalls = responses.filterIsInstance<com.lhzkml.jasmine.core.prompt.model.Message.Tool.Call>()
    if (toolCalls.isNotEmpty()) {
        action(toolCalls)
    }
}

/**
 * å¦‚æœ Message.Response åˆ—è¡¨åŒ…å« Assistant æ¶ˆæ¯ï¼Œæ‰§è¡?action
 * ç§»æ¤è‡?koog çš?onMultipleAssistantMessages(List<Message.Response>, (List<Message.Assistant>) -> Unit)
 */
@JvmName("onMultipleAssistantMessagesTyped")
inline fun AgentGraphContext.onMultipleAssistantMessages(
    responses: List<com.lhzkml.jasmine.core.prompt.model.Message.Response>,
    action: (List<com.lhzkml.jasmine.core.prompt.model.Message.Assistant>) -> Unit
) {
    val assistants = responses.filterIsInstance<com.lhzkml.jasmine.core.prompt.model.Message.Assistant>()
    if (assistants.isNotEmpty()) {
        action(assistants)
    }
}

/**
 * æ£€æŸ?Message.Response åˆ—è¡¨æ˜¯å¦åŒ…å« Tool.Call
 * ç§»æ¤è‡?koog çš?List<Message.Response>.containsToolCalls()
 */
@JvmName("containsToolCallsTyped")
fun List<com.lhzkml.jasmine.core.prompt.model.Message.Response>.containsToolCalls(): Boolean =
    any { it is com.lhzkml.jasmine.core.prompt.model.Message.Tool.Call }

/**
 * ä»?Message.Response åˆ—è¡¨ä¸­æå–æ‰€æœ?Tool.Call
 * ç§»æ¤è‡?koog çš?extractToolCalls(List<Message.Response>)
 */
fun AgentGraphContext.extractToolCallMessages(
    responses: List<com.lhzkml.jasmine.core.prompt.model.Message.Response>
): List<com.lhzkml.jasmine.core.prompt.model.Message.Tool.Call> =
    responses.filterIsInstance<com.lhzkml.jasmine.core.prompt.model.Message.Tool.Call>()

/**
 * å°?Message.Response è½¬ä¸º Message.Assistantï¼Œå¦‚æœä¸æ˜¯åˆ™è¿”å› null
 * ç§»æ¤è‡?koog çš?Message.Response.asAssistantMessageOrNull()
 */
fun com.lhzkml.jasmine.core.prompt.model.Message.Response.asAssistantOrNull(): com.lhzkml.jasmine.core.prompt.model.Message.Assistant? =
    this as? com.lhzkml.jasmine.core.prompt.model.Message.Assistant

/**
 * å°?Message.Response å¼ºåˆ¶è½¬ä¸º Message.Assistant
 * ç§»æ¤è‡?koog çš?Message.Response.asAssistantMessage()
 */
fun com.lhzkml.jasmine.core.prompt.model.Message.Response.asAssistant(): com.lhzkml.jasmine.core.prompt.model.Message.Assistant =
    this as com.lhzkml.jasmine.core.prompt.model.Message.Assistant

/**
 * è·å–æœ€æ–°çš„ token ç”¨é‡ï¼ˆä» prompt ä¸­è¯»å–ï¼‰
 * ç§»æ¤è‡?koog çš?latestTokenUsage() (ä»?prompt.latestTokenUsage è¯»å–)
 */
suspend fun AgentGraphContext.latestTokenUsageFromPrompt(): Int {
    return session.prompt.latestTokenUsage
}
